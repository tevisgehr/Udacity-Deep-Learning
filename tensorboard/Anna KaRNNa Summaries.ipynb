{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 11, 31,  2, 40, 25, 15, 46, 75, 19, 19, 19, 22, 31,  2,  2, 63,\n",
       "       46, 35, 31, 57, 23, 51, 23, 25, 66, 46, 31, 15, 25, 46, 31, 51, 51,\n",
       "       46, 31, 51, 23, 30, 25, 37, 46, 25, 80, 25, 15, 63, 46, 54, 10, 11,\n",
       "       31,  2,  2, 63, 46, 35, 31, 57, 23, 51, 63, 46, 23, 66, 46, 54, 10,\n",
       "       11, 31,  2,  2, 63, 46, 23, 10, 46, 23, 40, 66, 46, 70,  8, 10, 19,\n",
       "        8, 31, 63, 78, 19, 19,  0, 80, 25, 15, 63, 40, 11, 23, 10], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52, 11, 31,  2, 40, 25, 15, 46, 75, 19],\n",
       "       [13, 10, 56, 46, 11, 25, 46, 57, 70, 80],\n",
       "       [46, 29, 31, 40, 29, 11, 23, 10, 73, 46],\n",
       "       [70, 40, 11, 25, 15, 46,  8, 70, 54, 51],\n",
       "       [46, 40, 11, 25, 46, 51, 31, 10, 56, 32],\n",
       "       [46, 45, 11, 15, 70, 54, 73, 11, 46, 51],\n",
       "       [40, 46, 40, 70, 19, 56, 70, 78, 19, 19],\n",
       "       [70, 46, 11, 25, 15, 66, 25, 51, 35, 24],\n",
       "       [11, 31, 40, 46, 23, 66, 46, 40, 11, 25],\n",
       "       [25, 15, 66, 25, 51, 35, 46, 31, 10, 56]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/1786 Training loss: 4.4174 3.9116 sec/batch\n",
      "Epoch 1/1  Iteration 2/1786 Training loss: 4.3694 3.8208 sec/batch\n",
      "Epoch 1/1  Iteration 3/1786 Training loss: 4.1955 3.9654 sec/batch\n",
      "Epoch 1/1  Iteration 4/1786 Training loss: 4.4613 4.0938 sec/batch\n",
      "Epoch 1/1  Iteration 5/1786 Training loss: 4.4450 3.9096 sec/batch\n",
      "Epoch 1/1  Iteration 6/1786 Training loss: 4.3548 4.3771 sec/batch\n",
      "Epoch 1/1  Iteration 7/1786 Training loss: 4.2504 4.4884 sec/batch\n",
      "Epoch 1/1  Iteration 8/1786 Training loss: 4.1541 4.1393 sec/batch\n",
      "Epoch 1/1  Iteration 9/1786 Training loss: 4.0807 3.8604 sec/batch\n",
      "Epoch 1/1  Iteration 10/1786 Training loss: 4.0158 3.9993 sec/batch\n",
      "Epoch 1/1  Iteration 11/1786 Training loss: 3.9658 4.0186 sec/batch\n",
      "Epoch 1/1  Iteration 12/1786 Training loss: 3.9189 4.0475 sec/batch\n",
      "Epoch 1/1  Iteration 13/1786 Training loss: 3.8727 4.1696 sec/batch\n",
      "Epoch 1/1  Iteration 14/1786 Training loss: 3.8333 3.9572 sec/batch\n",
      "Epoch 1/1  Iteration 15/1786 Training loss: 3.8030 4.0423 sec/batch\n",
      "Epoch 1/1  Iteration 16/1786 Training loss: 3.7744 3.8547 sec/batch\n",
      "Epoch 1/1  Iteration 17/1786 Training loss: 3.7500 3.9104 sec/batch\n",
      "Epoch 1/1  Iteration 18/1786 Training loss: 3.7259 3.9654 sec/batch\n",
      "Epoch 1/1  Iteration 19/1786 Training loss: 3.7055 4.1686 sec/batch\n",
      "Epoch 1/1  Iteration 20/1786 Training loss: 3.6812 3.9730 sec/batch\n",
      "Epoch 1/1  Iteration 21/1786 Training loss: 3.6642 4.0171 sec/batch\n",
      "Epoch 1/1  Iteration 22/1786 Training loss: 3.6475 3.9497 sec/batch\n",
      "Epoch 1/1  Iteration 23/1786 Training loss: 3.6329 3.8874 sec/batch\n",
      "Epoch 1/1  Iteration 24/1786 Training loss: 3.6176 3.9606 sec/batch\n",
      "Epoch 1/1  Iteration 25/1786 Training loss: 3.6043 4.1644 sec/batch\n",
      "Epoch 1/1  Iteration 26/1786 Training loss: 3.5914 3.8833 sec/batch\n",
      "Epoch 1/1  Iteration 27/1786 Training loss: 3.5782 3.9339 sec/batch\n",
      "Epoch 1/1  Iteration 28/1786 Training loss: 3.5651 3.8812 sec/batch\n",
      "Epoch 1/1  Iteration 29/1786 Training loss: 3.5523 3.9032 sec/batch\n",
      "Epoch 1/1  Iteration 30/1786 Training loss: 3.5403 3.8751 sec/batch\n",
      "Epoch 1/1  Iteration 31/1786 Training loss: 3.5314 4.2112 sec/batch\n",
      "Epoch 1/1  Iteration 32/1786 Training loss: 3.5214 3.8538 sec/batch\n",
      "Epoch 1/1  Iteration 33/1786 Training loss: 3.5110 3.9772 sec/batch\n",
      "Epoch 1/1  Iteration 34/1786 Training loss: 3.5021 3.8967 sec/batch\n",
      "Epoch 1/1  Iteration 35/1786 Training loss: 3.4930 4.0231 sec/batch\n",
      "Epoch 1/1  Iteration 36/1786 Training loss: 3.4847 4.0299 sec/batch\n",
      "Epoch 1/1  Iteration 37/1786 Training loss: 3.4764 4.1803 sec/batch\n",
      "Epoch 1/1  Iteration 38/1786 Training loss: 3.4690 3.9094 sec/batch\n",
      "Epoch 1/1  Iteration 39/1786 Training loss: 3.4631 3.9957 sec/batch\n",
      "Epoch 1/1  Iteration 40/1786 Training loss: 3.4563 3.9666 sec/batch\n",
      "Epoch 1/1  Iteration 41/1786 Training loss: 3.4513 3.8258 sec/batch\n",
      "Epoch 1/1  Iteration 42/1786 Training loss: 3.4449 4.0582 sec/batch\n",
      "Epoch 1/1  Iteration 43/1786 Training loss: 3.4389 4.0999 sec/batch\n",
      "Epoch 1/1  Iteration 44/1786 Training loss: 3.4315 3.9773 sec/batch\n",
      "Epoch 1/1  Iteration 45/1786 Training loss: 3.4252 3.8964 sec/batch\n",
      "Epoch 1/1  Iteration 46/1786 Training loss: 3.4204 3.8453 sec/batch\n",
      "Epoch 1/1  Iteration 47/1786 Training loss: 3.4156 3.9820 sec/batch\n",
      "Epoch 1/1  Iteration 48/1786 Training loss: 3.4104 3.8986 sec/batch\n",
      "Epoch 1/1  Iteration 49/1786 Training loss: 3.4062 4.1619 sec/batch\n",
      "Epoch 1/1  Iteration 50/1786 Training loss: 3.4016 3.9863 sec/batch\n",
      "Epoch 1/1  Iteration 51/1786 Training loss: 3.3976 3.9247 sec/batch\n",
      "Epoch 1/1  Iteration 52/1786 Training loss: 3.3925 3.8677 sec/batch\n",
      "Epoch 1/1  Iteration 53/1786 Training loss: 3.3900 3.8728 sec/batch\n",
      "Epoch 1/1  Iteration 54/1786 Training loss: 3.3864 3.8686 sec/batch\n",
      "Epoch 1/1  Iteration 55/1786 Training loss: 3.3833 4.0644 sec/batch\n",
      "Epoch 1/1  Iteration 56/1786 Training loss: 3.3792 3.8346 sec/batch\n",
      "Epoch 1/1  Iteration 57/1786 Training loss: 3.3760 3.6302 sec/batch\n",
      "Epoch 1/1  Iteration 58/1786 Training loss: 3.3738 3.7893 sec/batch\n",
      "Epoch 1/1  Iteration 59/1786 Training loss: 3.3698 4.2073 sec/batch\n",
      "Epoch 1/1  Iteration 60/1786 Training loss: 3.3663 3.8554 sec/batch\n",
      "Epoch 1/1  Iteration 61/1786 Training loss: 3.3633 4.3596 sec/batch\n",
      "Epoch 1/1  Iteration 62/1786 Training loss: 3.3600 3.4048 sec/batch\n",
      "Epoch 1/1  Iteration 63/1786 Training loss: 3.3568 3.2134 sec/batch\n",
      "Epoch 1/1  Iteration 64/1786 Training loss: 3.3535 3.3280 sec/batch\n",
      "Epoch 1/1  Iteration 65/1786 Training loss: 3.3498 4.0848 sec/batch\n",
      "Epoch 1/1  Iteration 66/1786 Training loss: 3.3459 3.9082 sec/batch\n",
      "Epoch 1/1  Iteration 67/1786 Training loss: 3.3427 3.6524 sec/batch\n",
      "Epoch 1/1  Iteration 68/1786 Training loss: 3.3404 3.8304 sec/batch\n",
      "Epoch 1/1  Iteration 69/1786 Training loss: 3.3383 3.6829 sec/batch\n",
      "Epoch 1/1  Iteration 70/1786 Training loss: 3.3357 3.5791 sec/batch\n",
      "Epoch 1/1  Iteration 71/1786 Training loss: 3.3332 3.3410 sec/batch\n",
      "Epoch 1/1  Iteration 72/1786 Training loss: 3.3310 3.4076 sec/batch\n",
      "Epoch 1/1  Iteration 73/1786 Training loss: 3.3277 3.5301 sec/batch\n",
      "Epoch 1/1  Iteration 74/1786 Training loss: 3.3252 3.4279 sec/batch\n",
      "Epoch 1/1  Iteration 75/1786 Training loss: 3.3227 3.3945 sec/batch\n",
      "Epoch 1/1  Iteration 76/1786 Training loss: 3.3206 3.3599 sec/batch\n",
      "Epoch 1/1  Iteration 77/1786 Training loss: 3.3175 3.3538 sec/batch\n",
      "Epoch 1/1  Iteration 78/1786 Training loss: 3.3160 3.2963 sec/batch\n",
      "Epoch 1/1  Iteration 79/1786 Training loss: 3.3134 3.4822 sec/batch\n",
      "Epoch 1/1  Iteration 80/1786 Training loss: 3.3118 3.6127 sec/batch\n",
      "Epoch 1/1  Iteration 81/1786 Training loss: 3.3093 3.4730 sec/batch\n",
      "Epoch 1/1  Iteration 82/1786 Training loss: 3.3068 3.4019 sec/batch\n",
      "Epoch 1/1  Iteration 83/1786 Training loss: 3.3040 3.3682 sec/batch\n",
      "Epoch 1/1  Iteration 84/1786 Training loss: 3.3021 3.6608 sec/batch\n",
      "Epoch 1/1  Iteration 85/1786 Training loss: 3.3009 3.5091 sec/batch\n",
      "Epoch 1/1  Iteration 86/1786 Training loss: 3.2988 3.4542 sec/batch\n",
      "Epoch 1/1  Iteration 87/1786 Training loss: 3.2974 3.3909 sec/batch\n",
      "Epoch 1/1  Iteration 88/1786 Training loss: 3.2951 3.3584 sec/batch\n",
      "Epoch 1/1  Iteration 89/1786 Training loss: 3.2933 3.6241 sec/batch\n",
      "Epoch 1/1  Iteration 90/1786 Training loss: 3.2915 3.5127 sec/batch\n",
      "Epoch 1/1  Iteration 91/1786 Training loss: 3.2894 3.4248 sec/batch\n",
      "Epoch 1/1  Iteration 92/1786 Training loss: 3.2872 3.4660 sec/batch\n",
      "Epoch 1/1  Iteration 93/1786 Training loss: 3.2856 3.2901 sec/batch\n",
      "Epoch 1/1  Iteration 94/1786 Training loss: 3.2836 3.3185 sec/batch\n",
      "Epoch 1/1  Iteration 95/1786 Training loss: 3.2821 3.3160 sec/batch\n",
      "Epoch 1/1  Iteration 96/1786 Training loss: 3.2802 3.3416 sec/batch\n",
      "Epoch 1/1  Iteration 97/1786 Training loss: 3.2795 3.5242 sec/batch\n",
      "Epoch 1/1  Iteration 98/1786 Training loss: 3.2788 3.4706 sec/batch\n",
      "Epoch 1/1  Iteration 99/1786 Training loss: 3.2771 3.2940 sec/batch\n",
      "Epoch 1/1  Iteration 100/1786 Training loss: 3.2764 3.3926 sec/batch\n",
      "Validation loss: 3.10211 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 101/1786 Training loss: 3.2748 3.5084 sec/batch\n",
      "Epoch 1/1  Iteration 102/1786 Training loss: 3.2738 3.4747 sec/batch\n",
      "Epoch 1/1  Iteration 103/1786 Training loss: 3.2718 3.4030 sec/batch\n",
      "Epoch 1/1  Iteration 104/1786 Training loss: 3.2706 3.3236 sec/batch\n",
      "Epoch 1/1  Iteration 105/1786 Training loss: 3.2686 3.5219 sec/batch\n",
      "Epoch 1/1  Iteration 106/1786 Training loss: 3.2670 3.4429 sec/batch\n",
      "Epoch 1/1  Iteration 107/1786 Training loss: 3.2654 3.7362 sec/batch\n",
      "Epoch 1/1  Iteration 108/1786 Training loss: 3.2639 3.8596 sec/batch\n",
      "Epoch 1/1  Iteration 109/1786 Training loss: 3.2622 3.4609 sec/batch\n",
      "Epoch 1/1  Iteration 110/1786 Training loss: 3.2604 3.3335 sec/batch\n",
      "Epoch 1/1  Iteration 111/1786 Training loss: 3.2591 3.3207 sec/batch\n",
      "Epoch 1/1  Iteration 112/1786 Training loss: 3.2575 3.4258 sec/batch\n",
      "Epoch 1/1  Iteration 113/1786 Training loss: 3.2566 3.4659 sec/batch\n",
      "Epoch 1/1  Iteration 114/1786 Training loss: 3.2556 3.6061 sec/batch\n",
      "Epoch 1/1  Iteration 115/1786 Training loss: 3.2541 3.6860 sec/batch\n",
      "Epoch 1/1  Iteration 116/1786 Training loss: 3.2524 3.7726 sec/batch\n",
      "Epoch 1/1  Iteration 117/1786 Training loss: 3.2510 3.3731 sec/batch\n",
      "Epoch 1/1  Iteration 118/1786 Training loss: 3.2497 3.4635 sec/batch\n",
      "Epoch 1/1  Iteration 119/1786 Training loss: 3.2488 3.9741 sec/batch\n",
      "Epoch 1/1  Iteration 120/1786 Training loss: 3.2474 3.7570 sec/batch\n",
      "Epoch 1/1  Iteration 121/1786 Training loss: 3.2465 3.9478 sec/batch\n",
      "Epoch 1/1  Iteration 122/1786 Training loss: 3.2464 3.7745 sec/batch\n",
      "Epoch 1/1  Iteration 123/1786 Training loss: 3.2454 3.4556 sec/batch\n",
      "Epoch 1/1  Iteration 124/1786 Training loss: 3.2438 3.9317 sec/batch\n",
      "Epoch 1/1  Iteration 125/1786 Training loss: 3.2424 3.5029 sec/batch\n",
      "Epoch 1/1  Iteration 126/1786 Training loss: 3.2414 3.6585 sec/batch\n",
      "Epoch 1/1  Iteration 127/1786 Training loss: 3.2404 3.3940 sec/batch\n",
      "Epoch 1/1  Iteration 128/1786 Training loss: 3.2391 3.3133 sec/batch\n",
      "Epoch 1/1  Iteration 129/1786 Training loss: 3.2379 3.3888 sec/batch\n",
      "Epoch 1/1  Iteration 130/1786 Training loss: 3.2367 3.3143 sec/batch\n",
      "Epoch 1/1  Iteration 131/1786 Training loss: 3.2353 3.4555 sec/batch\n",
      "Epoch 1/1  Iteration 132/1786 Training loss: 3.2339 3.5722 sec/batch\n",
      "Epoch 1/1  Iteration 133/1786 Training loss: 3.2323 3.3998 sec/batch\n",
      "Epoch 1/1  Iteration 134/1786 Training loss: 3.2309 3.3745 sec/batch\n",
      "Epoch 1/1  Iteration 135/1786 Training loss: 3.2294 3.3859 sec/batch\n",
      "Epoch 1/1  Iteration 136/1786 Training loss: 3.2280 3.3669 sec/batch\n",
      "Epoch 1/1  Iteration 137/1786 Training loss: 3.2268 3.3878 sec/batch\n",
      "Epoch 1/1  Iteration 138/1786 Training loss: 3.2251 3.5308 sec/batch\n",
      "Epoch 1/1  Iteration 139/1786 Training loss: 3.2236 3.3682 sec/batch\n",
      "Epoch 1/1  Iteration 140/1786 Training loss: 3.2219 3.3509 sec/batch\n",
      "Epoch 1/1  Iteration 141/1786 Training loss: 3.2204 3.4738 sec/batch\n",
      "Epoch 1/1  Iteration 142/1786 Training loss: 3.2188 3.3546 sec/batch\n",
      "Epoch 1/1  Iteration 143/1786 Training loss: 3.2170 3.5485 sec/batch\n",
      "Epoch 1/1  Iteration 144/1786 Training loss: 3.2153 3.4643 sec/batch\n",
      "Epoch 1/1  Iteration 145/1786 Training loss: 3.2137 3.3063 sec/batch\n",
      "Epoch 1/1  Iteration 146/1786 Training loss: 3.2120 3.3861 sec/batch\n",
      "Epoch 1/1  Iteration 147/1786 Training loss: 3.2104 3.3882 sec/batch\n",
      "Epoch 1/1  Iteration 148/1786 Training loss: 3.2088 3.3152 sec/batch\n",
      "Epoch 1/1  Iteration 149/1786 Training loss: 3.2067 3.5505 sec/batch\n",
      "Epoch 1/1  Iteration 150/1786 Training loss: 3.2050 3.4762 sec/batch\n",
      "Epoch 1/1  Iteration 151/1786 Training loss: 3.2034 3.3797 sec/batch\n",
      "Epoch 1/1  Iteration 152/1786 Training loss: 3.2014 3.3453 sec/batch\n",
      "Epoch 1/1  Iteration 153/1786 Training loss: 3.1995 3.3438 sec/batch\n",
      "Epoch 1/1  Iteration 154/1786 Training loss: 3.1979 3.3519 sec/batch\n",
      "Epoch 1/1  Iteration 155/1786 Training loss: 3.1963 3.4289 sec/batch\n",
      "Epoch 1/1  Iteration 156/1786 Training loss: 3.1949 3.4884 sec/batch\n",
      "Epoch 1/1  Iteration 157/1786 Training loss: 3.1931 3.3802 sec/batch\n",
      "Epoch 1/1  Iteration 158/1786 Training loss: 3.1913 3.3791 sec/batch\n",
      "Epoch 1/1  Iteration 159/1786 Training loss: 3.1899 3.3287 sec/batch\n",
      "Epoch 1/1  Iteration 160/1786 Training loss: 3.1882 3.3867 sec/batch\n",
      "Epoch 1/1  Iteration 161/1786 Training loss: 3.1863 3.4147 sec/batch\n",
      "Epoch 1/1  Iteration 162/1786 Training loss: 3.1845 3.4799 sec/batch\n",
      "Epoch 1/1  Iteration 163/1786 Training loss: 3.1824 3.4262 sec/batch\n",
      "Epoch 1/1  Iteration 164/1786 Training loss: 3.1805 3.3299 sec/batch\n",
      "Epoch 1/1  Iteration 165/1786 Training loss: 3.1786 3.3009 sec/batch\n",
      "Epoch 1/1  Iteration 166/1786 Training loss: 3.1770 3.3974 sec/batch\n",
      "Epoch 1/1  Iteration 167/1786 Training loss: 3.1750 3.4477 sec/batch\n",
      "Epoch 1/1  Iteration 168/1786 Training loss: 3.1738 3.5480 sec/batch\n",
      "Epoch 1/1  Iteration 169/1786 Training loss: 3.1716 3.3647 sec/batch\n",
      "Epoch 1/1  Iteration 170/1786 Training loss: 3.1698 3.4060 sec/batch\n",
      "Epoch 1/1  Iteration 171/1786 Training loss: 3.1685 3.3678 sec/batch\n",
      "Epoch 1/1  Iteration 172/1786 Training loss: 3.1671 3.4269 sec/batch\n",
      "Epoch 1/1  Iteration 173/1786 Training loss: 3.1655 3.4790 sec/batch\n",
      "Epoch 1/1  Iteration 174/1786 Training loss: 3.1638 3.5127 sec/batch\n",
      "Epoch 1/1  Iteration 175/1786 Training loss: 3.1617 3.4023 sec/batch\n",
      "Epoch 1/1  Iteration 176/1786 Training loss: 3.1600 3.3862 sec/batch\n",
      "Epoch 1/1  Iteration 177/1786 Training loss: 3.1584 3.3459 sec/batch\n",
      "Epoch 1/1  Iteration 178/1786 Training loss: 3.1566 3.8393 sec/batch\n",
      "Epoch 1/1  Iteration 179/1786 Training loss: 3.1549 3.4673 sec/batch\n",
      "Epoch 1/1  Iteration 180/1786 Training loss: 3.1526 3.5655 sec/batch\n",
      "Epoch 1/1  Iteration 181/1786 Training loss: 3.1508 3.5788 sec/batch\n",
      "Epoch 1/1  Iteration 182/1786 Training loss: 3.1491 3.8251 sec/batch\n",
      "Epoch 1/1  Iteration 183/1786 Training loss: 3.1471 3.3316 sec/batch\n",
      "Epoch 1/1  Iteration 184/1786 Training loss: 3.1450 3.4027 sec/batch\n",
      "Epoch 1/1  Iteration 185/1786 Training loss: 3.1428 3.3943 sec/batch\n",
      "Epoch 1/1  Iteration 186/1786 Training loss: 3.1406 3.5483 sec/batch\n",
      "Epoch 1/1  Iteration 187/1786 Training loss: 3.1381 3.4042 sec/batch\n",
      "Epoch 1/1  Iteration 188/1786 Training loss: 3.1361 3.3636 sec/batch\n",
      "Epoch 1/1  Iteration 189/1786 Training loss: 3.1339 3.3977 sec/batch\n",
      "Epoch 1/1  Iteration 190/1786 Training loss: 3.1314 3.3352 sec/batch\n",
      "Epoch 1/1  Iteration 191/1786 Training loss: 3.1291 3.4449 sec/batch\n",
      "Epoch 1/1  Iteration 192/1786 Training loss: 3.1273 3.5443 sec/batch\n",
      "Epoch 1/1  Iteration 193/1786 Training loss: 3.1252 3.3984 sec/batch\n",
      "Epoch 1/1  Iteration 194/1786 Training loss: 3.1231 3.7785 sec/batch\n",
      "Epoch 1/1  Iteration 195/1786 Training loss: 3.1207 3.4166 sec/batch\n",
      "Epoch 1/1  Iteration 196/1786 Training loss: 3.1183 3.2975 sec/batch\n",
      "Epoch 1/1  Iteration 197/1786 Training loss: 3.1163 3.4415 sec/batch\n",
      "Epoch 1/1  Iteration 198/1786 Training loss: 3.1141 3.5372 sec/batch\n",
      "Epoch 1/1  Iteration 199/1786 Training loss: 3.1120 3.4446 sec/batch\n",
      "Epoch 1/1  Iteration 200/1786 Training loss: 3.1095 3.3606 sec/batch\n",
      "Validation loss: 2.64934 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 201/1786 Training loss: 3.1069 2.4315 sec/batch\n",
      "Epoch 1/1  Iteration 202/1786 Training loss: 3.1047 2.5646 sec/batch\n",
      "Epoch 1/1  Iteration 203/1786 Training loss: 3.1023 2.3456 sec/batch\n",
      "Epoch 1/1  Iteration 204/1786 Training loss: 3.1001 2.4704 sec/batch\n",
      "Epoch 1/1  Iteration 205/1786 Training loss: 3.0979 2.4481 sec/batch\n",
      "Epoch 1/1  Iteration 206/1786 Training loss: 3.0957 2.4455 sec/batch\n",
      "Epoch 1/1  Iteration 207/1786 Training loss: 3.0935 2.4978 sec/batch\n",
      "Epoch 1/1  Iteration 208/1786 Training loss: 3.0910 2.4971 sec/batch\n",
      "Epoch 1/1  Iteration 209/1786 Training loss: 3.0885 2.4089 sec/batch\n",
      "Epoch 1/1  Iteration 210/1786 Training loss: 3.0860 2.4429 sec/batch\n",
      "Epoch 1/1  Iteration 211/1786 Training loss: 3.0838 2.3943 sec/batch\n",
      "Epoch 1/1  Iteration 212/1786 Training loss: 3.0816 2.4784 sec/batch\n",
      "Epoch 1/1  Iteration 213/1786 Training loss: 3.0793 2.5123 sec/batch\n",
      "Epoch 1/1  Iteration 214/1786 Training loss: 3.0767 2.4350 sec/batch\n",
      "Epoch 1/1  Iteration 215/1786 Training loss: 3.0742 2.4062 sec/batch\n",
      "Epoch 1/1  Iteration 216/1786 Training loss: 3.0716 2.4406 sec/batch\n",
      "Epoch 1/1  Iteration 217/1786 Training loss: 3.0691 2.4222 sec/batch\n",
      "Epoch 1/1  Iteration 218/1786 Training loss: 3.0665 2.4903 sec/batch\n",
      "Epoch 1/1  Iteration 219/1786 Training loss: 3.0640 2.5655 sec/batch\n",
      "Epoch 1/1  Iteration 220/1786 Training loss: 3.0616 2.4165 sec/batch\n",
      "Epoch 1/1  Iteration 221/1786 Training loss: 3.0593 2.4446 sec/batch\n",
      "Epoch 1/1  Iteration 222/1786 Training loss: 3.0570 2.4273 sec/batch\n",
      "Epoch 1/1  Iteration 223/1786 Training loss: 3.0546 2.3463 sec/batch\n",
      "Epoch 1/1  Iteration 224/1786 Training loss: 3.0522 2.5386 sec/batch\n",
      "Epoch 1/1  Iteration 225/1786 Training loss: 3.0499 2.4377 sec/batch\n",
      "Epoch 1/1  Iteration 226/1786 Training loss: 3.0476 2.5141 sec/batch\n",
      "Epoch 1/1  Iteration 227/1786 Training loss: 3.0452 2.8030 sec/batch\n",
      "Epoch 1/1  Iteration 228/1786 Training loss: 3.0430 2.4045 sec/batch\n",
      "Epoch 1/1  Iteration 229/1786 Training loss: 3.0409 2.4415 sec/batch\n",
      "Epoch 1/1  Iteration 230/1786 Training loss: 3.0388 2.5308 sec/batch\n",
      "Epoch 1/1  Iteration 231/1786 Training loss: 3.0363 2.4723 sec/batch\n",
      "Epoch 1/1  Iteration 232/1786 Training loss: 3.0340 2.4230 sec/batch\n",
      "Epoch 1/1  Iteration 233/1786 Training loss: 3.0319 2.4209 sec/batch\n",
      "Epoch 1/1  Iteration 234/1786 Training loss: 3.0293 2.4395 sec/batch\n",
      "Epoch 1/1  Iteration 235/1786 Training loss: 3.0267 2.4551 sec/batch\n",
      "Epoch 1/1  Iteration 236/1786 Training loss: 3.0246 2.5484 sec/batch\n",
      "Epoch 1/1  Iteration 237/1786 Training loss: 3.0225 2.3848 sec/batch\n",
      "Epoch 1/1  Iteration 238/1786 Training loss: 3.0204 2.4301 sec/batch\n",
      "Epoch 1/1  Iteration 239/1786 Training loss: 3.0182 2.4348 sec/batch\n",
      "Epoch 1/1  Iteration 240/1786 Training loss: 3.0159 2.3821 sec/batch\n",
      "Epoch 1/1  Iteration 241/1786 Training loss: 3.0135 2.4759 sec/batch\n",
      "Epoch 1/1  Iteration 242/1786 Training loss: 3.0115 2.4641 sec/batch\n",
      "Epoch 1/1  Iteration 243/1786 Training loss: 3.0095 2.4018 sec/batch\n",
      "Epoch 1/1  Iteration 244/1786 Training loss: 3.0076 2.4177 sec/batch\n",
      "Epoch 1/1  Iteration 245/1786 Training loss: 3.0058 2.3783 sec/batch\n",
      "Epoch 1/1  Iteration 246/1786 Training loss: 3.0037 2.4219 sec/batch\n",
      "Epoch 1/1  Iteration 247/1786 Training loss: 3.0018 2.5304 sec/batch\n",
      "Epoch 1/1  Iteration 248/1786 Training loss: 2.9996 2.4055 sec/batch\n",
      "Epoch 1/1  Iteration 249/1786 Training loss: 2.9973 2.4185 sec/batch\n",
      "Epoch 1/1  Iteration 250/1786 Training loss: 2.9955 2.4049 sec/batch\n",
      "Epoch 1/1  Iteration 251/1786 Training loss: 2.9933 2.4418 sec/batch\n",
      "Epoch 1/1  Iteration 252/1786 Training loss: 2.9910 2.4510 sec/batch\n",
      "Epoch 1/1  Iteration 253/1786 Training loss: 2.9887 2.4625 sec/batch\n",
      "Epoch 1/1  Iteration 254/1786 Training loss: 2.9867 2.4095 sec/batch\n",
      "Epoch 1/1  Iteration 255/1786 Training loss: 2.9847 2.4600 sec/batch\n",
      "Epoch 1/1  Iteration 256/1786 Training loss: 2.9829 2.4325 sec/batch\n",
      "Epoch 1/1  Iteration 257/1786 Training loss: 2.9809 2.4133 sec/batch\n",
      "Epoch 1/1  Iteration 258/1786 Training loss: 2.9791 2.4122 sec/batch\n",
      "Epoch 1/1  Iteration 259/1786 Training loss: 2.9771 2.4850 sec/batch\n",
      "Epoch 1/1  Iteration 260/1786 Training loss: 2.9753 2.3869 sec/batch\n",
      "Epoch 1/1  Iteration 261/1786 Training loss: 2.9732 2.3784 sec/batch\n",
      "Epoch 1/1  Iteration 262/1786 Training loss: 2.9711 2.4160 sec/batch\n",
      "Epoch 1/1  Iteration 263/1786 Training loss: 2.9691 2.3983 sec/batch\n",
      "Epoch 1/1  Iteration 264/1786 Training loss: 2.9670 2.5207 sec/batch\n",
      "Epoch 1/1  Iteration 265/1786 Training loss: 2.9649 2.4305 sec/batch\n",
      "Epoch 1/1  Iteration 266/1786 Training loss: 2.9630 2.3701 sec/batch\n",
      "Epoch 1/1  Iteration 267/1786 Training loss: 2.9608 3.0180 sec/batch\n",
      "Epoch 1/1  Iteration 268/1786 Training loss: 2.9588 2.9441 sec/batch\n",
      "Epoch 1/1  Iteration 269/1786 Training loss: 2.9562 2.6694 sec/batch\n",
      "Epoch 1/1  Iteration 270/1786 Training loss: 2.9538 3.1704 sec/batch\n",
      "Epoch 1/1  Iteration 271/1786 Training loss: 2.9519 2.8435 sec/batch\n",
      "Epoch 1/1  Iteration 272/1786 Training loss: 2.9498 3.1821 sec/batch\n",
      "Epoch 1/1  Iteration 273/1786 Training loss: 2.9479 2.9287 sec/batch\n",
      "Epoch 1/1  Iteration 274/1786 Training loss: 2.9462 2.5555 sec/batch\n",
      "Epoch 1/1  Iteration 275/1786 Training loss: 2.9441 2.2517 sec/batch\n",
      "Epoch 1/1  Iteration 276/1786 Training loss: 2.9431 2.2479 sec/batch\n",
      "Epoch 1/1  Iteration 277/1786 Training loss: 2.9412 2.7166 sec/batch\n",
      "Epoch 1/1  Iteration 278/1786 Training loss: 2.9395 2.7637 sec/batch\n",
      "Epoch 1/1  Iteration 279/1786 Training loss: 2.9374 2.5018 sec/batch\n",
      "Epoch 1/1  Iteration 280/1786 Training loss: 2.9358 2.6501 sec/batch\n",
      "Epoch 1/1  Iteration 281/1786 Training loss: 2.9342 2.4945 sec/batch\n",
      "Epoch 1/1  Iteration 282/1786 Training loss: 2.9325 2.3761 sec/batch\n",
      "Epoch 1/1  Iteration 283/1786 Training loss: 2.9308 2.3125 sec/batch\n",
      "Epoch 1/1  Iteration 284/1786 Training loss: 2.9292 2.2945 sec/batch\n",
      "Epoch 1/1  Iteration 285/1786 Training loss: 2.9280 2.3233 sec/batch\n",
      "Epoch 1/1  Iteration 286/1786 Training loss: 2.9264 2.2783 sec/batch\n",
      "Epoch 1/1  Iteration 287/1786 Training loss: 2.9246 2.3481 sec/batch\n",
      "Epoch 1/1  Iteration 288/1786 Training loss: 2.9226 2.2469 sec/batch\n",
      "Epoch 1/1  Iteration 289/1786 Training loss: 2.9210 2.2173 sec/batch\n",
      "Epoch 1/1  Iteration 290/1786 Training loss: 2.9196 2.2462 sec/batch\n",
      "Epoch 1/1  Iteration 291/1786 Training loss: 2.9180 2.2529 sec/batch\n",
      "Epoch 1/1  Iteration 292/1786 Training loss: 2.9166 2.2590 sec/batch\n",
      "Epoch 1/1  Iteration 293/1786 Training loss: 2.9148 2.3173 sec/batch\n",
      "Epoch 1/1  Iteration 294/1786 Training loss: 2.9129 2.2093 sec/batch\n",
      "Epoch 1/1  Iteration 295/1786 Training loss: 2.9117 2.2009 sec/batch\n",
      "Epoch 1/1  Iteration 296/1786 Training loss: 2.9101 2.2195 sec/batch\n",
      "Epoch 1/1  Iteration 297/1786 Training loss: 2.9088 2.1964 sec/batch\n",
      "Epoch 1/1  Iteration 298/1786 Training loss: 2.9075 2.2902 sec/batch\n",
      "Epoch 1/1  Iteration 299/1786 Training loss: 2.9059 2.2325 sec/batch\n",
      "Epoch 1/1  Iteration 300/1786 Training loss: 2.9040 2.2367 sec/batch\n",
      "Validation loss: 2.41822 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 301/1786 Training loss: 2.9025 2.9867 sec/batch\n",
      "Epoch 1/1  Iteration 302/1786 Training loss: 2.9009 2.3468 sec/batch\n",
      "Epoch 1/1  Iteration 303/1786 Training loss: 2.8990 2.7201 sec/batch\n",
      "Epoch 1/1  Iteration 304/1786 Training loss: 2.8975 2.3563 sec/batch\n",
      "Epoch 1/1  Iteration 305/1786 Training loss: 2.8962 2.3933 sec/batch\n",
      "Epoch 1/1  Iteration 306/1786 Training loss: 2.8946 2.5156 sec/batch\n",
      "Epoch 1/1  Iteration 307/1786 Training loss: 2.8931 2.4341 sec/batch\n",
      "Epoch 1/1  Iteration 308/1786 Training loss: 2.8913 2.3968 sec/batch\n",
      "Epoch 1/1  Iteration 309/1786 Training loss: 2.8896 2.6495 sec/batch\n",
      "Epoch 1/1  Iteration 310/1786 Training loss: 2.8877 2.2241 sec/batch\n",
      "Epoch 1/1  Iteration 311/1786 Training loss: 2.8860 2.2667 sec/batch\n",
      "Epoch 1/1  Iteration 312/1786 Training loss: 2.8845 2.3941 sec/batch\n",
      "Epoch 1/1  Iteration 313/1786 Training loss: 2.8827 2.3885 sec/batch\n",
      "Epoch 1/1  Iteration 314/1786 Training loss: 2.8810 2.3252 sec/batch\n",
      "Epoch 1/1  Iteration 315/1786 Training loss: 2.8797 2.2550 sec/batch\n",
      "Epoch 1/1  Iteration 316/1786 Training loss: 2.8781 2.2146 sec/batch\n",
      "Epoch 1/1  Iteration 317/1786 Training loss: 2.8766 2.2817 sec/batch\n",
      "Epoch 1/1  Iteration 318/1786 Training loss: 2.8754 2.3648 sec/batch\n",
      "Epoch 1/1  Iteration 319/1786 Training loss: 2.8738 2.2000 sec/batch\n",
      "Epoch 1/1  Iteration 320/1786 Training loss: 2.8719 2.3330 sec/batch\n",
      "Epoch 1/1  Iteration 321/1786 Training loss: 2.8705 2.2736 sec/batch\n",
      "Epoch 1/1  Iteration 322/1786 Training loss: 2.8688 2.2874 sec/batch\n",
      "Epoch 1/1  Iteration 323/1786 Training loss: 2.8672 2.3012 sec/batch\n",
      "Epoch 1/1  Iteration 324/1786 Training loss: 2.8656 2.2318 sec/batch\n",
      "Epoch 1/1  Iteration 325/1786 Training loss: 2.8641 2.2186 sec/batch\n",
      "Epoch 1/1  Iteration 326/1786 Training loss: 2.8624 2.2074 sec/batch\n",
      "Epoch 1/1  Iteration 327/1786 Training loss: 2.8609 2.2332 sec/batch\n",
      "Epoch 1/1  Iteration 328/1786 Training loss: 2.8593 2.2390 sec/batch\n",
      "Epoch 1/1  Iteration 329/1786 Training loss: 2.8578 2.3167 sec/batch\n",
      "Epoch 1/1  Iteration 330/1786 Training loss: 2.8564 2.2077 sec/batch\n",
      "Epoch 1/1  Iteration 331/1786 Training loss: 2.8550 2.2585 sec/batch\n",
      "Epoch 1/1  Iteration 332/1786 Training loss: 2.8538 2.2221 sec/batch\n",
      "Epoch 1/1  Iteration 333/1786 Training loss: 2.8526 2.2999 sec/batch\n",
      "Epoch 1/1  Iteration 334/1786 Training loss: 2.8512 2.3496 sec/batch\n",
      "Epoch 1/1  Iteration 335/1786 Training loss: 2.8498 2.2803 sec/batch\n",
      "Epoch 1/1  Iteration 336/1786 Training loss: 2.8485 2.2553 sec/batch\n",
      "Epoch 1/1  Iteration 337/1786 Training loss: 2.8475 2.2185 sec/batch\n",
      "Epoch 1/1  Iteration 338/1786 Training loss: 2.8458 2.2102 sec/batch\n",
      "Epoch 1/1  Iteration 339/1786 Training loss: 2.8446 2.2402 sec/batch\n",
      "Epoch 1/1  Iteration 340/1786 Training loss: 2.8432 2.3663 sec/batch\n",
      "Epoch 1/1  Iteration 341/1786 Training loss: 2.8417 2.2931 sec/batch\n",
      "Epoch 1/1  Iteration 342/1786 Training loss: 2.8401 2.2569 sec/batch\n",
      "Epoch 1/1  Iteration 343/1786 Training loss: 2.8385 2.2979 sec/batch\n",
      "Epoch 1/1  Iteration 344/1786 Training loss: 2.8368 2.3439 sec/batch\n",
      "Epoch 1/1  Iteration 345/1786 Training loss: 2.8355 2.5880 sec/batch\n",
      "Epoch 1/1  Iteration 346/1786 Training loss: 2.8341 2.5043 sec/batch\n",
      "Epoch 1/1  Iteration 347/1786 Training loss: 2.8329 2.4573 sec/batch\n",
      "Epoch 1/1  Iteration 348/1786 Training loss: 2.8314 2.2466 sec/batch\n",
      "Epoch 1/1  Iteration 349/1786 Training loss: 2.8303 2.6045 sec/batch\n",
      "Epoch 1/1  Iteration 350/1786 Training loss: 2.8289 2.4345 sec/batch\n",
      "Epoch 1/1  Iteration 351/1786 Training loss: 2.8277 2.8191 sec/batch\n",
      "Epoch 1/1  Iteration 352/1786 Training loss: 2.8265 2.9506 sec/batch\n",
      "Epoch 1/1  Iteration 353/1786 Training loss: 2.8253 3.3392 sec/batch\n",
      "Epoch 1/1  Iteration 354/1786 Training loss: 2.8239 2.9121 sec/batch\n",
      "Epoch 1/1  Iteration 355/1786 Training loss: 2.8226 2.8020 sec/batch\n",
      "Epoch 1/1  Iteration 356/1786 Training loss: 2.8212 2.5109 sec/batch\n",
      "Epoch 1/1  Iteration 357/1786 Training loss: 2.8199 2.5026 sec/batch\n",
      "Epoch 1/1  Iteration 358/1786 Training loss: 2.8183 2.5420 sec/batch\n",
      "Epoch 1/1  Iteration 359/1786 Training loss: 2.8169 2.3479 sec/batch\n",
      "Epoch 1/1  Iteration 360/1786 Training loss: 2.8155 2.7119 sec/batch\n",
      "Epoch 1/1  Iteration 361/1786 Training loss: 2.8141 2.7280 sec/batch\n",
      "Epoch 1/1  Iteration 362/1786 Training loss: 2.8129 2.5140 sec/batch\n",
      "Epoch 1/1  Iteration 363/1786 Training loss: 2.8114 3.2287 sec/batch\n",
      "Epoch 1/1  Iteration 364/1786 Training loss: 2.8100 2.8238 sec/batch\n",
      "Epoch 1/1  Iteration 365/1786 Training loss: 2.8086 2.5734 sec/batch\n",
      "Epoch 1/1  Iteration 366/1786 Training loss: 2.8073 2.7312 sec/batch\n",
      "Epoch 1/1  Iteration 367/1786 Training loss: 2.8058 3.2070 sec/batch\n",
      "Epoch 1/1  Iteration 368/1786 Training loss: 2.8046 3.1207 sec/batch\n",
      "Epoch 1/1  Iteration 369/1786 Training loss: 2.8034 3.1294 sec/batch\n",
      "Epoch 1/1  Iteration 370/1786 Training loss: 2.8023 2.4487 sec/batch\n",
      "Epoch 1/1  Iteration 371/1786 Training loss: 2.8008 3.0733 sec/batch\n",
      "Epoch 1/1  Iteration 372/1786 Training loss: 2.7995 2.6579 sec/batch\n",
      "Epoch 1/1  Iteration 373/1786 Training loss: 2.7983 2.9799 sec/batch\n",
      "Epoch 1/1  Iteration 374/1786 Training loss: 2.7971 2.4502 sec/batch\n",
      "Epoch 1/1  Iteration 375/1786 Training loss: 2.7959 2.3252 sec/batch\n",
      "Epoch 1/1  Iteration 376/1786 Training loss: 2.7949 2.4508 sec/batch\n",
      "Epoch 1/1  Iteration 377/1786 Training loss: 2.7935 2.1862 sec/batch\n",
      "Epoch 1/1  Iteration 378/1786 Training loss: 2.7920 2.2511 sec/batch\n",
      "Epoch 1/1  Iteration 379/1786 Training loss: 2.7908 2.3158 sec/batch\n",
      "Epoch 1/1  Iteration 380/1786 Training loss: 2.7896 2.3052 sec/batch\n",
      "Epoch 1/1  Iteration 381/1786 Training loss: 2.7884 2.1857 sec/batch\n",
      "Epoch 1/1  Iteration 382/1786 Training loss: 2.7872 2.2403 sec/batch\n",
      "Epoch 1/1  Iteration 383/1786 Training loss: 2.7862 2.2290 sec/batch\n",
      "Epoch 1/1  Iteration 384/1786 Training loss: 2.7850 2.4059 sec/batch\n",
      "Epoch 1/1  Iteration 385/1786 Training loss: 2.7838 2.4712 sec/batch\n",
      "Epoch 1/1  Iteration 386/1786 Training loss: 2.7824 2.5429 sec/batch\n",
      "Epoch 1/1  Iteration 387/1786 Training loss: 2.7816 2.6753 sec/batch\n",
      "Epoch 1/1  Iteration 388/1786 Training loss: 2.7803 2.3060 sec/batch\n",
      "Epoch 1/1  Iteration 389/1786 Training loss: 2.7793 2.2392 sec/batch\n",
      "Epoch 1/1  Iteration 390/1786 Training loss: 2.7782 2.2577 sec/batch\n",
      "Epoch 1/1  Iteration 391/1786 Training loss: 2.7768 2.3685 sec/batch\n",
      "Epoch 1/1  Iteration 392/1786 Training loss: 2.7756 2.2521 sec/batch\n",
      "Epoch 1/1  Iteration 393/1786 Training loss: 2.7746 2.3014 sec/batch\n",
      "Epoch 1/1  Iteration 394/1786 Training loss: 2.7733 2.3419 sec/batch\n",
      "Epoch 1/1  Iteration 395/1786 Training loss: 2.7719 2.2817 sec/batch\n",
      "Epoch 1/1  Iteration 396/1786 Training loss: 2.7705 2.4562 sec/batch\n",
      "Epoch 1/1  Iteration 397/1786 Training loss: 2.7693 2.4328 sec/batch\n",
      "Epoch 1/1  Iteration 398/1786 Training loss: 2.7680 2.4496 sec/batch\n",
      "Epoch 1/1  Iteration 399/1786 Training loss: 2.7667 2.4308 sec/batch\n",
      "Epoch 1/1  Iteration 400/1786 Training loss: 2.7654 2.3637 sec/batch\n",
      "Validation loss: 2.27971 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 401/1786 Training loss: 2.7640 2.4196 sec/batch\n",
      "Epoch 1/1  Iteration 402/1786 Training loss: 2.7629 2.3311 sec/batch\n",
      "Epoch 1/1  Iteration 403/1786 Training loss: 2.7618 2.8388 sec/batch\n",
      "Epoch 1/1  Iteration 404/1786 Training loss: 2.7604 2.6178 sec/batch\n",
      "Epoch 1/1  Iteration 405/1786 Training loss: 2.7592 2.5899 sec/batch\n",
      "Epoch 1/1  Iteration 406/1786 Training loss: 2.7580 2.3256 sec/batch\n",
      "Epoch 1/1  Iteration 407/1786 Training loss: 2.7567 2.2840 sec/batch\n",
      "Epoch 1/1  Iteration 408/1786 Training loss: 2.7553 2.4204 sec/batch\n",
      "Epoch 1/1  Iteration 409/1786 Training loss: 2.7540 2.7269 sec/batch\n",
      "Epoch 1/1  Iteration 410/1786 Training loss: 2.7528 2.7470 sec/batch\n",
      "Epoch 1/1  Iteration 411/1786 Training loss: 2.7516 2.2898 sec/batch\n",
      "Epoch 1/1  Iteration 412/1786 Training loss: 2.7503 2.2305 sec/batch\n",
      "Epoch 1/1  Iteration 413/1786 Training loss: 2.7492 2.6651 sec/batch\n",
      "Epoch 1/1  Iteration 414/1786 Training loss: 2.7480 2.7514 sec/batch\n",
      "Epoch 1/1  Iteration 415/1786 Training loss: 2.7468 2.3656 sec/batch\n",
      "Epoch 1/1  Iteration 416/1786 Training loss: 2.7456 2.3376 sec/batch\n",
      "Epoch 1/1  Iteration 417/1786 Training loss: 2.7444 2.7259 sec/batch\n",
      "Epoch 1/1  Iteration 418/1786 Training loss: 2.7433 2.8330 sec/batch\n",
      "Epoch 1/1  Iteration 419/1786 Training loss: 2.7419 3.2099 sec/batch\n",
      "Epoch 1/1  Iteration 420/1786 Training loss: 2.7408 2.9118 sec/batch\n",
      "Epoch 1/1  Iteration 421/1786 Training loss: 2.7398 2.6720 sec/batch\n",
      "Epoch 1/1  Iteration 422/1786 Training loss: 2.7386 2.7398 sec/batch\n",
      "Epoch 1/1  Iteration 423/1786 Training loss: 2.7372 2.5718 sec/batch\n",
      "Epoch 1/1  Iteration 424/1786 Training loss: 2.7361 2.3375 sec/batch\n",
      "Epoch 1/1  Iteration 425/1786 Training loss: 2.7349 2.4854 sec/batch\n",
      "Epoch 1/1  Iteration 426/1786 Training loss: 2.7339 2.3693 sec/batch\n",
      "Epoch 1/1  Iteration 427/1786 Training loss: 2.7328 2.8255 sec/batch\n",
      "Epoch 1/1  Iteration 428/1786 Training loss: 2.7318 2.8739 sec/batch\n",
      "Epoch 1/1  Iteration 429/1786 Training loss: 2.7308 2.5092 sec/batch\n",
      "Epoch 1/1  Iteration 430/1786 Training loss: 2.7299 2.2605 sec/batch\n",
      "Epoch 1/1  Iteration 431/1786 Training loss: 2.7287 2.3219 sec/batch\n",
      "Epoch 1/1  Iteration 432/1786 Training loss: 2.7277 2.4778 sec/batch\n",
      "Epoch 1/1  Iteration 433/1786 Training loss: 2.7268 2.3993 sec/batch\n",
      "Epoch 1/1  Iteration 434/1786 Training loss: 2.7259 2.4895 sec/batch\n",
      "Epoch 1/1  Iteration 435/1786 Training loss: 2.7249 3.0801 sec/batch\n",
      "Epoch 1/1  Iteration 436/1786 Training loss: 2.7238 2.6916 sec/batch\n",
      "Epoch 1/1  Iteration 437/1786 Training loss: 2.7226 2.9431 sec/batch\n",
      "Epoch 1/1  Iteration 438/1786 Training loss: 2.7216 2.7399 sec/batch\n",
      "Epoch 1/1  Iteration 439/1786 Training loss: 2.7205 2.5087 sec/batch\n",
      "Epoch 1/1  Iteration 440/1786 Training loss: 2.7192 2.8515 sec/batch\n",
      "Epoch 1/1  Iteration 441/1786 Training loss: 2.7183 2.7248 sec/batch\n",
      "Epoch 1/1  Iteration 442/1786 Training loss: 2.7173 3.2266 sec/batch\n",
      "Epoch 1/1  Iteration 443/1786 Training loss: 2.7161 2.5609 sec/batch\n",
      "Epoch 1/1  Iteration 444/1786 Training loss: 2.7149 2.5610 sec/batch\n",
      "Epoch 1/1  Iteration 445/1786 Training loss: 2.7138 2.4882 sec/batch\n",
      "Epoch 1/1  Iteration 446/1786 Training loss: 2.7127 2.2946 sec/batch\n",
      "Epoch 1/1  Iteration 447/1786 Training loss: 2.7116 2.6714 sec/batch\n",
      "Epoch 1/1  Iteration 448/1786 Training loss: 2.7104 2.4815 sec/batch\n",
      "Epoch 1/1  Iteration 449/1786 Training loss: 2.7094 2.3984 sec/batch\n",
      "Epoch 1/1  Iteration 450/1786 Training loss: 2.7082 2.3722 sec/batch\n",
      "Epoch 1/1  Iteration 451/1786 Training loss: 2.7070 2.5506 sec/batch\n",
      "Epoch 1/1  Iteration 452/1786 Training loss: 2.7059 2.4658 sec/batch\n",
      "Epoch 1/1  Iteration 453/1786 Training loss: 2.7049 2.5244 sec/batch\n",
      "Epoch 1/1  Iteration 454/1786 Training loss: 2.7041 2.4429 sec/batch\n",
      "Epoch 1/1  Iteration 455/1786 Training loss: 2.7033 2.7484 sec/batch\n",
      "Epoch 1/1  Iteration 456/1786 Training loss: 2.7022 3.1570 sec/batch\n",
      "Epoch 1/1  Iteration 457/1786 Training loss: 2.7011 2.4363 sec/batch\n",
      "Epoch 1/1  Iteration 458/1786 Training loss: 2.7002 2.5506 sec/batch\n",
      "Epoch 1/1  Iteration 459/1786 Training loss: 2.6991 2.4186 sec/batch\n",
      "Epoch 1/1  Iteration 460/1786 Training loss: 2.6978 2.4800 sec/batch\n",
      "Epoch 1/1  Iteration 461/1786 Training loss: 2.6967 2.6376 sec/batch\n",
      "Epoch 1/1  Iteration 462/1786 Training loss: 2.6956 2.7413 sec/batch\n",
      "Epoch 1/1  Iteration 463/1786 Training loss: 2.6944 2.3419 sec/batch\n",
      "Epoch 1/1  Iteration 464/1786 Training loss: 2.6934 3.0245 sec/batch\n",
      "Epoch 1/1  Iteration 465/1786 Training loss: 2.6922 2.9286 sec/batch\n",
      "Epoch 1/1  Iteration 466/1786 Training loss: 2.6912 2.2650 sec/batch\n",
      "Epoch 1/1  Iteration 467/1786 Training loss: 2.6901 2.1995 sec/batch\n",
      "Epoch 1/1  Iteration 468/1786 Training loss: 2.6891 2.4139 sec/batch\n",
      "Epoch 1/1  Iteration 469/1786 Training loss: 2.6880 3.0959 sec/batch\n",
      "Epoch 1/1  Iteration 470/1786 Training loss: 2.6868 3.0614 sec/batch\n",
      "Epoch 1/1  Iteration 471/1786 Training loss: 2.6857 2.6872 sec/batch\n",
      "Epoch 1/1  Iteration 472/1786 Training loss: 2.6846 2.7937 sec/batch\n",
      "Epoch 1/1  Iteration 473/1786 Training loss: 2.6836 3.0321 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9d286771a6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n\u001b[1;32m     37\u001b[0m                                                           model.final_state, model.optimizer], \n\u001b[0;32m---> 38\u001b[0;31m                                                           feed_dict=feed)\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Applications/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
