{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73, 45, 31,  2, 62, 25, 17, 38, 44, 46, 46, 46, 18, 31,  2,  2, 66,\n",
       "       38,  1, 31, 81, 39, 61, 39, 25, 57, 38, 31, 17, 25, 38, 31, 61, 61,\n",
       "       38, 31, 61, 39, 75, 25, 49, 38, 25, 22, 25, 17, 66, 38, 54,  0, 45,\n",
       "       31,  2,  2, 66, 38,  1, 31, 81, 39, 61, 66, 38, 39, 57, 38, 54,  0,\n",
       "       45, 31,  2,  2, 66, 38, 39,  0, 38, 39, 62, 57, 38, 20, 30,  0, 46,\n",
       "       30, 31, 66, 82, 46, 46, 63, 22, 25, 17, 66, 62, 45, 39,  0], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73, 45, 31,  2, 62, 25, 17, 38, 44, 46],\n",
       "       [27,  0,  4, 38, 45, 25, 38, 81, 20, 22],\n",
       "       [38, 47, 31, 62, 47, 45, 39,  0, 77, 38],\n",
       "       [20, 62, 45, 25, 17, 38, 30, 20, 54, 61],\n",
       "       [38, 62, 45, 25, 38, 61, 31,  0,  4, 64],\n",
       "       [38, 55, 45, 17, 20, 54, 77, 45, 38, 61],\n",
       "       [62, 38, 62, 20, 46,  4, 20, 82, 46, 46],\n",
       "       [20, 38, 45, 25, 17, 57, 25, 61,  1, 78],\n",
       "       [45, 31, 62, 38, 39, 57, 38, 62, 45, 25],\n",
       "       [25, 17, 57, 25, 61,  1, 38, 31,  0,  4]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    \n",
    "    seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                           name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4172 7.4726 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3646 6.8088 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.1658 6.3709 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.0509 6.5977 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 3.9489 6.7300 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 3.8610 7.1383 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 3.7905 7.1475 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 3.7346 7.6222 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 3.6882 8.0197 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 3.6498 7.5991 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 3.6150 7.0193 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.5843 6.5204 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.5573 6.7677 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.5349 6.2226 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.5142 6.3748 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.4969 6.5618 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.4805 6.8845 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.4668 7.4428 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.4534 7.3770 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.4397 6.5961 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.4280 6.5978 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.4174 6.9668 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.4077 6.4544 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.3984 6.1808 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.3896 6.3723 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.3820 6.4804 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.3748 6.9161 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.3672 6.2470 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.3605 6.4634 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.3542 6.3345 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.3491 6.8779 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.3433 6.8831 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.3374 6.7402 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.3327 6.6445 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.3275 6.8553 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.3231 6.4213 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.3179 6.4299 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.3134 7.3682 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.3088 7.5170 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.3045 7.2057 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.3004 6.6647 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.2965 6.5510 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.2925 6.6921 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.2888 6.6401 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.2850 6.6520 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.2817 7.0914 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.2786 6.7547 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.2756 7.3468 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.2730 6.6902 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.2702 6.6794 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.2673 6.5846 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.2644 6.6356 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.2617 6.5993 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.2589 6.5957 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.2563 6.9654 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.2534 6.6684 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.2508 6.6932 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.2482 6.6280 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.2455 6.6140 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.2432 6.6636 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.2594 6.6439 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.2632 6.6096 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.2613 6.6252 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.2585 6.6200 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.2561 6.5936 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.2541 6.5897 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.2520 6.7906 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.2493 7.1541 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.2469 7.2443 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.2449 6.6151 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.2427 6.6048 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.2409 6.6585 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.2389 7.1895 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.2369 6.6436 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.2352 6.6849 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.2334 6.7283 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.2316 6.6257 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.2297 6.6616 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.2278 6.6183 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.2257 6.6093 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.2238 6.5519 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.2221 6.6112 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.2203 6.6164 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.2185 6.6503 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.2165 6.5972 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.2146 6.6660 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.2126 6.6357 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.2107 6.5698 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.2090 6.6429 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.2073 6.6200 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.2055 6.6486 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.2036 6.6342 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.2016 6.5930 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.1999 6.6067 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.1982 6.6327 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.1963 6.6311 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.1946 6.6580 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.1928 6.6179 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.1910 6.5565 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.1892 7.0093 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.1874 7.1833 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.1856 7.6888 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.1837 7.5252 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.1817 7.6341 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.1797 6.6197 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.1778 6.6292 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.1756 6.6211 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.1735 6.6183 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.1714 6.5864 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.1689 6.5731 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.1666 6.6470 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.1643 6.6568 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.1619 6.6611 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.1593 6.6965 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.1566 6.6515 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.1539 6.5780 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.1513 6.6872 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.1488 6.6429 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.1464 6.6153 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.1437 6.7431 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.1413 7.3557 sec/batch\n",
      "Epoch 1/1  Iteration 122/178 Training loss: 3.1387 7.5874 sec/batch\n",
      "Epoch 1/1  Iteration 123/178 Training loss: 3.1359 8.6250 sec/batch\n",
      "Epoch 1/1  Iteration 124/178 Training loss: 3.1334 7.5579 sec/batch\n",
      "Epoch 1/1  Iteration 125/178 Training loss: 3.1306 8.9257 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.1276 8.4815 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.1249 12.5660 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.1222 15.8144 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.1193 17.8344 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.1164 15.0928 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.1135 11.9130 sec/batch\n",
      "Epoch 1/1  Iteration 132/178 Training loss: 3.1103 14.9298 sec/batch\n",
      "Epoch 1/1  Iteration 133/178 Training loss: 3.1073 23.5317 sec/batch\n",
      "Epoch 1/1  Iteration 134/178 Training loss: 3.1043 24.9622 sec/batch\n",
      "Epoch 1/1  Iteration 135/178 Training loss: 3.1009 24.2665 sec/batch\n",
      "Epoch 1/1  Iteration 136/178 Training loss: 3.0977 24.0791 sec/batch\n",
      "Epoch 1/1  Iteration 137/178 Training loss: 3.0946 23.7749 sec/batch\n",
      "Epoch 1/1  Iteration 138/178 Training loss: 3.0914 24.0188 sec/batch\n",
      "Epoch 1/1  Iteration 139/178 Training loss: 3.0883 24.0805 sec/batch\n",
      "Epoch 1/1  Iteration 140/178 Training loss: 3.0851 24.1921 sec/batch\n",
      "Epoch 1/1  Iteration 141/178 Training loss: 3.0821 23.8441 sec/batch\n",
      "Epoch 1/1  Iteration 142/178 Training loss: 3.0790 23.5399 sec/batch\n",
      "Epoch 1/1  Iteration 143/178 Training loss: 3.0758 21.8516 sec/batch\n",
      "Epoch 1/1  Iteration 144/178 Training loss: 3.0728 21.7440 sec/batch\n",
      "Epoch 1/1  Iteration 145/178 Training loss: 3.0697 20.4405 sec/batch\n",
      "Epoch 1/1  Iteration 146/178 Training loss: 3.0667 21.0795 sec/batch\n",
      "Epoch 1/1  Iteration 147/178 Training loss: 3.0635 21.1247 sec/batch\n",
      "Epoch 1/1  Iteration 148/178 Training loss: 3.0606 20.7368 sec/batch\n",
      "Epoch 1/1  Iteration 149/178 Training loss: 3.0573 20.3624 sec/batch\n",
      "Epoch 1/1  Iteration 150/178 Training loss: 3.0541 20.2552 sec/batch\n",
      "Epoch 1/1  Iteration 151/178 Training loss: 3.0511 20.5102 sec/batch\n",
      "Epoch 1/1  Iteration 152/178 Training loss: 3.0482 20.6746 sec/batch\n",
      "Epoch 1/1  Iteration 153/178 Training loss: 3.0451 20.3183 sec/batch\n",
      "Epoch 1/1  Iteration 154/178 Training loss: 3.0420 20.3071 sec/batch\n",
      "Epoch 1/1  Iteration 155/178 Training loss: 3.0388 20.2187 sec/batch\n",
      "Epoch 1/1  Iteration 156/178 Training loss: 3.0356 22.5941 sec/batch\n",
      "Epoch 1/1  Iteration 157/178 Training loss: 3.0323 22.1865 sec/batch\n",
      "Epoch 1/1  Iteration 158/178 Training loss: 3.0290 23.5509 sec/batch\n",
      "Epoch 1/1  Iteration 159/178 Training loss: 3.0257 21.3397 sec/batch\n",
      "Epoch 1/1  Iteration 160/178 Training loss: 3.0225 20.8524 sec/batch\n",
      "Epoch 1/1  Iteration 161/178 Training loss: 3.0194 21.1322 sec/batch\n",
      "Epoch 1/1  Iteration 162/178 Training loss: 3.0161 21.6071 sec/batch\n",
      "Epoch 1/1  Iteration 163/178 Training loss: 3.0128 22.8090 sec/batch\n",
      "Epoch 1/1  Iteration 164/178 Training loss: 3.0096 20.8497 sec/batch\n",
      "Epoch 1/1  Iteration 165/178 Training loss: 3.0066 20.5210 sec/batch\n",
      "Epoch 1/1  Iteration 166/178 Training loss: 3.0034 20.4714 sec/batch\n",
      "Epoch 1/1  Iteration 167/178 Training loss: 3.0004 20.4799 sec/batch\n",
      "Epoch 1/1  Iteration 168/178 Training loss: 2.9973 20.3580 sec/batch\n",
      "Epoch 1/1  Iteration 169/178 Training loss: 2.9943 20.7347 sec/batch\n",
      "Epoch 1/1  Iteration 170/178 Training loss: 2.9912 20.4492 sec/batch\n",
      "Epoch 1/1  Iteration 171/178 Training loss: 2.9882 20.4244 sec/batch\n",
      "Epoch 1/1  Iteration 172/178 Training loss: 2.9853 21.0504 sec/batch\n",
      "Epoch 1/1  Iteration 173/178 Training loss: 2.9826 21.4480 sec/batch\n",
      "Epoch 1/1  Iteration 174/178 Training loss: 2.9798 20.6590 sec/batch\n",
      "Epoch 1/1  Iteration 175/178 Training loss: 2.9771 20.9347 sec/batch\n",
      "Epoch 1/1  Iteration 176/178 Training loss: 2.9741 20.4055 sec/batch\n",
      "Epoch 1/1  Iteration 177/178 Training loss: 2.9711 22.4796 sec/batch\n",
      "Epoch 1/1  Iteration 178/178 Training loss: 2.9680 21.2404 sec/batch\n",
      "Validation loss: 2.36381 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.364.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.364.ckpt\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlying thees wot ang al he ans thes he alt or wethan side sorint of hererensis wimeresintin his wos the soren of ant oner wan tasdet ils has thas whes ane he worin tho hin wing, wothing the touthe hed the simthe with sithe arer ous wo wins ans tothe he sede the wim he his sh so whe the to she toun hised hot he to sher on whand te he sotin hither ans hese silt tor th aretas on al tore th whase here tor ther afeter sasithe and ant tins af hee hase we wot te sanseres ot at tar ins an wertint thind tim han sate tous an ane soustond wase al af oto tote he the hered\n",
      "at oulid,. she tor of to he soulle the toren the saned af ond ting he tas then an he sorerer the with whe ther thar heas he the sares or whas,\n",
      "ade sime hond sote tangerigg althasese tan sot or wothe sersithe he serin he sheres thens thingerens an he wit the sereran ad ato sin one te hor anses ons, tastithe his tor wand the the ale wat hos torens ans out that he she sering ond an tire ther ans an the tharesd ande wore and terersos ansenthe sis tor ins of hang whang he ton touth ang ont and tho hand with he woring he che thas andithe he anedisg th me tinte he he thim ho herat an oud we th won hed the her aresith, tha sader had sesith out ont ar the he and withad he that al he the als aled on on othe was ate thing anthon ated he caresad af the che tith tand the withe thos sad an ofe af to meeresin af ansile wha shared hhing af ther an har asing thes\n",
      "atishe he thim hin the win ald wont the thim ha the he we selt ardint hhas thar the hit tisger an atet ing on he this an whin af ane tor the taredes than to mithing af tor aferis he simer th thon tho win ate al il as an ore tith sethe and, an sas and otore sous and on al orerare thing at was he he whis wars hes wot the wing tot he wet ofe wo the there has an walt on he sherar wet he wot the hersend ofet, to me werent te tout ons wist the tho sared ther tor he sithe want wh tan and toung th ta the se we than he sas antor the thit one he sased af wonte tha gesise setisg i\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i178_l512_2.364.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard tifhes afdrerer ins of he cis the wer sont ane has he and, to sothes and ond to tir or af the sasd and we tit the thet tout hed somtin thes he hed afe an him an her wos the her atinntis hor he astot hes on thin ter alse tho cheresed the cer ife we sha san tare shes thind and the woud siting wat herisser of thint, the se toud son to the the wese to tot heres thering onthe the horend. Taut whe sarsad ote ton tous hor ther heres and ale and thereres at orate ant if ther soril san ont it wher had ansed tha carin oth the ansed in tor ile tor of wout there sot out aner wer athin thice sothe hot ha the hered he cererat ind, hit wer outhe sot te we he soril othe he shinsed wass and whis he te an whe wared had and on his the cat af oustha he hime shon he afis har was an he ther tans tas itis the wotho ter whon we whing the sas he the werens ofe se the he seser ate\n",
      "toun so han sing th an tore sorigh sas tor ithe won hes thot he were sit and wot hos ther wass intore tort te more hareringe whe he\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i178_l512_2.364.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farde nfarin soton to th the sin on tore to misg tous tont ot tere ta sete sont af her teretis and wim this,\n",
      "aft if hate hise the tan an tome herereren tho ho seres ind thot thes hime whe the tin whe hersere anthe the to the sing whate sore has and he thend\n",
      "tithin so tarit ha therarer touthed. And had the che he hha sit in hit hirint to thet his se toth thing one tor at he and and af han serithen he whan that hh there aresin as timhese hid and and thins, ho sarise sis than at te wat ha cerase sath ase with the her he hors whor wher an her hot oned af ter ifersentin tho se thon wot hed and wes tere tire the withan toot hot an winge at anes the then sis an oned he sartin an hit ale touth sas ore seter and he he sosinn ans hestin the thes af wor he are al af tous, whit wald witthe wath we whe tha sar afd out te sete an tho hhande whes hes of ont out hos and and on thit hout hin and tant, thin tho the sose sithe and wheng, whins tans withan af an ane thes tha wos herite shase hor an onte het h\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i178_l512_2.364.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlingig ante hithes arad in he her hare an he serestorer the sis hon se won wha this seterit welsensint the alse ther wise th ane wors, whad, sed ante thimisgis hat hhe thing ande an of wout, tha ge hed wans oth wime the who he than hesd anderers ind an tho the sont he the sedrand af his hime wered hims\n",
      "ansore the sas therersand ons hise tit an thes thass\n",
      "oned and the hed sor on the he wond whis sosisg thete har afe te hire thet ant oneteressens ont ther taring tho he ariled\n",
      "tat he the ton the soud she hint to sothed ha the sisles ofe het her wis sh tomerat an af the cereres tore her tate wersent herseding ther the tise thictesen whet al he tothe ho har who cone so hith sore tas he so she son hadedidet in he has and ot win him whe wo f hit hite sith sosthim af tint of tat he win has he hed hins here sas the thes af the sast ato ho te sot the se soreren whand hins at he sers hhe he the hor wat teule to who chan wint has\n",
      "tond wos toun hit the an tore tout, whin had and wer there the whang \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i178_l512_2.364.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
